import os
from glob import glob
from tqdm.notebook import tqdm
import numpy as np
import random
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import timm

import imagesize

try:
    from cuml import TSNE # if gpu is ON
except:
    from sklearn.manifold import TSNE # for cpu
    
import wandb
import IPython.display as ipd

class CFG:
    seed          = 42
    base_path     = './input/happy-whale-and-dolphin'
    embed_path    = './input/happywhale-embedding-dataset'
    num_samples   = None #  None for all samples
    device        = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    competition   = 'happywhale'
    _wandb_kernel = 'awsaf49'


def seed_torch(seed_value):
    random.seed(seed_value) # Python
    np.random.seed(seed_value) # cpu vars
    torch.manual_seed(seed_value) # cpu  vars    
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value) # gpu vars
    if torch.backends.cudnn.is_available:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print('# SEEDING DONE')
seed_torch(CFG.seed)

# if you wanna use wandb, use it
# try:
#     user_secrets = '72822190acbfe6e32f48e9c652274c39aea9187b'
#     api_key = user_secrets
#     wandb.login(key=api_key)
#     anonymous = None
#     print('ì„±ê³µí•¨')

# except:
#     anonymous = "must"
#     wandb.login(anonymous=anonymous)
#     print('To use your W&B account,\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \nGet your W&B access token from here: https://wandb.ai/authorize')



df = pd.read_csv(f'{CFG.base_path}/train.csv')
df['image_path'] = CFG.base_path+'/train_images/'+df['image']
df['split'] = 'Train'

test_df = pd.read_csv(f'{CFG.base_path}/sample_submission.csv')
test_df['image_path'] = CFG.base_path+'/test_images/'+test_df['image']
test_df['split'] = 'Test'

# print('Train Images: {:,} | Test Images: {:,}'.format(len(df), len(test_df)))


# convert beluga, globis to whales
df.loc[df.species.str.contains('beluga'), 'species'] = 'beluga_whale'
df.loc[df.species.str.contains('globis'), 'species'] = 'globis_whale'

df['class'] = df.species.map(lambda x: 'whale' if 'whale' in x else 'dolphin')
# ë¬¸ìí˜• ê°’ë“¤ì´ mapí•¨ìˆ˜ë¥¼ ê±°ì¹˜ë©´ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ë¨
# ex) A, B, C, C, B -> 1, 2, 3, 3, 2


# fix duplicate(ë³µì œí•˜ë‹¤) labels
# https://www.kaggle.com/c/happy-whale-and-dolphin/discussion/304633
df['species'] = df['species'].str.replace('bottlenose_dolpin','bottlenose_dolphin')
df['species'] = df['species'].str.replace('kiler_whale','killer_whale')

# Find Image Size
def get_imgsize(row):
    row['width'], row['height'] = imagesize.get(row['image_path'])
    return row

# Train
tqdm.pandas(desc='Train ')
df = df.progress_apply(get_imgsize, axis=1)
print('Train:')
ipd.display(df.head(2))

# Test
tqdm.pandas(desc='Test ')
test_df = test_df.progress_apply(get_imgsize, axis=1)
print('Test:')
ipd.display(test_df.head(2))



if CFG.num_samples:
    df = df.iloc[:CFG.num_samples]
    test_df = test_df.iloc[:CFG.num_samples]


data = df.species.value_counts().reset_index()
# print(data) ì´ê±°í•˜ë©´ ê° ì¸ë±ìŠ¤ë³„ë¡œ ëª‡ê°œìˆëŠ”ì§€ ë‚˜ì˜´
fig = px.bar(data, x='index', y='species', color='species',title='Species', text_auto=True)
# color ì—†ì• ë©´ ë˜‘ê°™ì€ ìƒ‰ê°ˆë¡œë‚˜ì˜¤ë„¤
fig.update_traces(textfont_size=12, textangle=0, textposition="outside", cliponaxis=False)
# fig.show()
# bar chartë¥¼ ë§Œë“¤ì–´ëƒ„



data = df['class'].value_counts().reset_index()

fig = px.bar(data, x='index', y='class', color='class', title='Whale Vs Dolphin', text_auto=True)
fig.update_traces(textfont_size=12, textangle=0, textposition="outside", cliponaxis=False)
# fig.show()
# https://www.kaggle.com/awsaf49/happywhale-data-distribution

# print(df)


# width, height ë¡œ EDAë¥¼ í•  ìˆ˜ ìˆìŒ class vs iamgesize
# ì´ë¯¸ì§€ì˜ ë¶„í¬ê°€ ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ ì´ë¤„ì§€ëŠ”ì§€
# It is visible that Distribution of ImageSize is similar for both Whale and Dolphin except some cases in height.

fig = px.histogram(df,
                   x="width", 
                   color="class",
                   barmode='group',
                   log_y=True,
                   title='Width Vs Class')
# fig.show()

fig = px.histogram(df,
                   x="height", 
                   color="class",   
                   barmode='group',
                   log_y=True,
                   title='Height Vs Class')
# fig.show()


# ImageSize Vs Split(Train/Test)
# It can be notices that distribution of width for train and test data, looks quite similar. So, we can resize without any tension.
# For height we have some unique shapes.

fig = px.histogram(pd.concat([df, test_df]),
                   x="width", 
                   color="split",
                   barmode='group',
                   log_y=True,
                   title='Width Vs Split')
# fig.show()

fig = px.histogram(pd.concat([df, test_df]),
                   x="height", 
                   color="split",
                   barmode='group',
                   log_y=True,
                   title='Height Vs Split')
# fig.show()


# Data Pipeline ğŸš
# To create image embedding we will,  (embedding -> ê³ ì°¨ì›ì„ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜(from high dimension to low dimension))
# Read the image.
# Resize it accordingly.

def load_image(path):
    img = cv2.imread(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    # opencv ê°€ BGRë¡œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— RGB ì±„ë„ë¡œ ë³€í™˜
    return img


class ImageDataset(Dataset):
    def __init__(self,
                 path,
                 target=None,
                 input_shape=(128, 256),
                #  input_shapeì— í° ì˜ë¯¸ì—†ìŒ ë‚˜ì¤‘ì— 224, 224ë¡œ ë°›ì•„ì˜´
                 transform=None,
                 channel_first=True,
                ):
        super(ImageDataset, self).__init__()
        # super().__init__()
        # super()ë¡œ ê¸°ë°˜ í´ë˜ìŠ¤(ë¶€ëª¨ í´ë˜ìŠ¤)ë¥¼ ì´ˆê¸°í™”í•´ì¤Œìœ¼ë¡œì¨, ê¸°ë°˜ í´ë˜ìŠ¤ì˜ ì†ì„±ì„ subclassê°€ ë°›ì•„ì˜¤ë„ë¡ í•œë‹¤. 
        # (ì´ˆê¸°í™”ë¥¼ í•˜ì§€ ì•Šìœ¼ë©´, ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ì†ì„±ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ)

        self.path = path
        self.target = target
        self.input_shape = input_shape
        self.transform = transform
        self.channel_first = channel_first

    def __len__(self):
        return len(self.path)
    
    def __getitem__(self, idx):
        img = load_image(self.path[idx])
        img = cv2.resize(img, dsize=self.input_shape)
        if self.transform is not None:
            img = self.transform(image=img)["image"]
        if self.channel_first:
            img = img.transpose((2, 0, 1))
        if self.target is not None:
            target = self.target[idx]
            return img, target
        else:
            return img

def get_dataset(path, target=None, batch_size=32, input_shape=(224, 224)):

    # ë‚˜ì¤‘ì— batch_sizeë„ configë¡œ ê³ ì¹ ìˆ˜ ìˆê² ë‹¤.
    dataset = ImageDataset(path=path,
                           target=target,
                           input_shape=input_shape,
                          )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=2,
        # num_workerê°€ 2ë¡œ ë˜ìˆì—ˆëŠ”ë°, 0ìœ¼ë¡œ ê³ ì¹˜ë©´ dataloaderì—ì„œ error ì•ˆë‚¨
        # num_workerëŠ” gpuë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì“°ê¸°ìœ„í•´ ìˆëŠ”ê±¸ë¡œì•”. í”„ë¡œì íŠ¸ ë‹¤ ëë‚˜ë©´ num_worker = 2 ë¡œ í•´ë³¼ê²ƒ
        shuffle=False,
        pin_memory=True,
    )
    return dataloader


# ì´ê±´ì•„ì§ ì•ˆë´¤ìŒ visualization í•  ë•Œ í•„ìš”
def plot_batch(batch, row=2, col=2, channel_first=True):
    if isinstance(batch, tuple) or isinstance(batch, list):
        imgs, tars = batch
    else:
        imgs, tars = batch, None
    plt.figure(figsize=(col*3, row*3))
    for i in range(row*col):
        plt.subplot(row, col, i+1)
        img = imgs[i].numpy()
        if channel_first:
            img = img.transpose((1, 2, 0))
        plt.imshow(img)
        if tars is not None:
            plt.title(tars[i])
        plt.axis('off')
    plt.tight_layout()
    # plt.show()
    

def gen_colors(n=10):
    cmap   = plt.get_cmap('rainbow')
    colors = [cmap(i) for i in np.linspace(0, 1, n + 2)]
    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]
    return colors



if __name__ == '__main__':
    
    train_loader = get_dataset(path=df.image_path.tolist(),
                        target=df.species.tolist(),
                        input_shape=(224,224),
                        )
                        
    test_loader = get_dataset(path=test_df.image_path.tolist(),
                        target=None,
                        input_shape=(224,224),
                        )


    batch = iter(train_loader).next()
    plot_batch(batch, row=2, col=5)

    batch = iter(test_loader).next()
    plot_batch(batch, row=2, col=5)


    class ImageModel(nn.Module):
        def __init__(self, backbone_name, pretrained=True):
            super(ImageModel, self).__init__()
            self.backbone = timm.create_model(backbone_name,
                                            pretrained=pretrained)
            self.backbone.reset_classifier(0) # to get pooled features
            #   classificationì— ê´€ë ¨ëœ layerë“¤ ì—†ì•° 
            #   í–ˆì„ë•Œ -> ì•ˆí–ˆì„ë•Œ
            #   (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
            #   (classifier): Linear(in_features=1280, out_features=1000, bias=True)

            #   (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
            #   (classifier): Identity()
            #   ì—¬ê¸°ì„œ outputìœ¼ë¡œ 1280ê°œì˜ ë²¡í„°ê°€ ìƒì„±ë¨.

        def forward(self, x):            
            x = self.backbone(x)
            return x

    model = ImageModel('tf_efficientnet_b0')

    @torch.no_grad()
    # @ pythonì— ëŒ€í•œ ì„¤ëª…ì€ https://choice-life.tistory.com/42 ì—¬ê¸° ì˜ ë‚˜ì™€ìˆìŠµë‹ˆë‹¤.

    def predict(model, dataloader):
        model.eval() # turn off layers such as BatchNorm or Dropout
        model.to(CFG.device) # cpu -> gpu
        embeds = []
        pbar = tqdm(dataloader, total=len(dataloader))
        for img in pbar:
            img = img.type(torch.float32) # uint8 -> float32
            img = img.to(CFG.device) # cpu -> gpu
            embed = model(img) # this is where magic happens ;)
            gpu_mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0
            pbar.set_postfix(gpu_mem=f'{gpu_mem:0.2f} GB')
            embeds.append(embed.cpu().detach().numpy())
        return np.concatenate(embeds)

    @torch.no_grad()
    def predict(model, dataloader):
        model.eval() # turn off layers such as BatchNorm or Dropout
        # ì´ê²Œ ì—†ìœ¼ë©´ batchnormì´ë‘ dropout ì•ˆêº¼ì§„ ìƒíƒœì—ì„œ ì§„í–‰ë¨
        model.to(CFG.device) # cpu -> gpu
        embeds = []
        pbar = tqdm(dataloader, total=len(dataloader))
        for img in pbar:
            img = img.type(torch.float32) # uint8 -> float32
            img = img.to(CFG.device) # cpu -> gpu
            embed = model(img) # this is where magic happens ;)
            gpu_mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0
            pbar.set_postfix(gpu_mem=f'{gpu_mem:0.2f} GB')
            # ì´ê±° ì£¼ì„ì²˜ë¦¬í–ˆì„ë•Œë‘ ì£¼ì„ì²˜ë¦¬ ì•ˆí–ˆì„ë•Œë‘ ì°¨ì´ë³´ê¸°..
            # set_postfixê°€ ë­”ì§€ ì˜ ëª¨ë¥´ê² ë„¤ìš”.
            embeds.append(embed.cpu().detach().numpy())
        return np.concatenate(embeds)


    train_loader = get_dataset(
        path=df.image_path.tolist(),
        target=None,
        input_shape=(224,224),
        batch_size=128,
        # 128*4 ì—ì„œ 128ë¡œì¤„ì˜€ìŒ
    )

    test_loader = get_dataset(
        path=test_df.image_path.tolist(),
        target=None,
        input_shape=(224,224),
        batch_size=128*3,
        # 128*4 ì—ì„œ 128ë¡œì¤„ì˜€ìŒ
    )

    # if CFG.embed_path:
    #     print('# Train Embeddings:')
    #     train_embeds = np.load(f'{CFG.embed_path}/train_embeds.npy')
        
    #     print('# Test Embeddings:')
    #     test_embeds = np.load(f'{CFG.embed_path}/test_embeds.npy')
        
    # else:
    print('# Train Embeddings:')
    train_embeds = predict(model, train_loader)
    np.save(f'{CFG.embed_path}/train_embeds.npy', train_embeds) # save embeddings for reuse

    print('# Test Embeddings:')
    test_embeds = predict(model, test_loader)
    np.save(f'{CFG.embed_path}/train_embeds.npy', test_embeds) # save embeddings for reuse


    tsne = TSNE()

    # Concatenate both train and test
    embeds = np.concatenate([train_embeds,test_embeds])

    # Fit TSNE on the embeddings and then transfer data
    embed2D = tsne.fit_transform(embeds)

    print(embed2D)
    print(embed2D.shape)

    # Train
    df['x'] = embed2D[:len(train_embeds),0]
    df['y'] = embed2D[:len(train_embeds),1]

    # Test
    test_df['x'] = embed2D[len(train_embeds):,0]
    test_df['y'] = embed2D[len(train_embeds):,1]


    # convert config from class to dict
    config = {k:v for k,v in dict(vars(CFG)).items() if '__' not in k}
    # configíŒŒì¼ __main__, __getitem__ ê°™ì€ í•„ìš”ì—†ëŠ”ê²ƒë“¤ì€ ì œê±°í•˜ê³  dictionaryí˜•íƒœë¡œ ì €ì¥í•¨
    # varsëŠ” ê´€ë ¨ëœ ëª¨ë“ (?) ì •ë³´ë¥¼ ë±‰ì–´ëƒ„ ã…

    # initialize wandb project
    wandb.init(project='happywhale-public', config=config)

    # process data for wandb
    wdf1 = pd.concat([df, test_df]).drop(columns=['image_path','predictions']) # train + test
    wdf2 = df.copy() # only train as some columns of test don't have any value e.g: species

    # log the data
    wandb.log({"All":wdf1, 
            "Train":wdf2}) # log both result

    # save embeddings to wandb for later use
    wandb.save('test_embeds.npy'); # save train embeddings
    wandb.save('train_embeds.npy'); # save test embeddings

    # show wandb dashboard
    ipd.display(ipd.IFrame(wandb.run.url, width=1080, height=720)) # show wandb dashboard

    # finish logging
    wandb.finish()
    

    